---
title: "STA 663 Final Project: Bayesian Hierarchical Clustering"
author: "Sam Voisin & Jonathan Klus"
date: "April 30, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Bayesian Hierarchical Clustering (BHC) is an agglomerative tree-based method for identifying underlying population structures ("clusters"). BHC was introduced by K. Heller and Z. Ghahramani as a way to approximate the more computationally intensive Dirichlet process mixture model with Gaussian components introduced by C. Rassmusen. The advantage of these models over their counterparts lies in the fact that an *ex ante* number of clusters does not need to be specified. Instead, the Bayesian approach allows for regularized flexibility via a prior placed on the cluster concentration parameter $\alpha$. We utilized the object-oriented programing (OOP) capabilities to build a module that can be used similar manner to the popular scikit-learn library. As with many Bayesian methods, the increased flexibility of BHC comes at a computational cost and the increased risk of poor results due to misspecified priors. Throughout this paper we will describe methods we used to mitigate these issues where possible.

## Background

The paper we chose to analyze and reproduce is *Bayesian Hierarchical Clustering* by Katherine Heller and Zoubin Ghahramani.

Cluster analysis is a frequent problem encountered in statistics/ probability and machine learning disciplines. It involves assigning observed data to discrete groups based on shared or similar attributes. Hierarchical clustering is a particular subset of methods of clustering that take into account the *degree* to which data points are similar. Data points that are more similar are clustered first. Then clusters that are less similar are grouped until only one all-encompassing cluster remains. This is also known as *agglomerative clustering*.

The frequentist hierarchical clustering method requires the user to specify the number of clusters prior to the actual clustering process. This is potentially problematic because the true number of clusters that exist within the data distribution is not usually known. This problem is addressed by BHC through the use of a cluster concentration parameter that regularizes, rather than restricts, the number of possible data clusters.

There are two key limitations to the BHC algorithm according to K. Heller. The first limitation is the computational cost which is $O(n^2)$ using Big-O notation. This is caused by the algorithm's need to compare every possible pair of clusters during each iteration. The second limitation of BHC is the lack of any incorporation of tree uncertainty which is the uncertainty of merging two clusters.

## The Algorithm

The core of the BHC algorithm relies on a Bayesian hypothesis test in which two alternatives are compared:

1) $H_1$ is the hypothesis that two clusters $D_i$ and $D_j$ were generated from the same distribution $p(x | \theta)$ with the prior distribution for $\theta$ being $p(\theta | \beta)$. The probability of clusters $i$ and $j$ being generated from the same distribution is defined as $p(D_k|H_1)$. The posterior for this hypothesis is:

$$
r_k = \frac{\pi_k p(D_k|H_1)}{\pi_k p(D_k|H_1) + (1 - \pi_k) p(D_i|T_i)p(D_j|T_j)}
$$

Note that $\pi_k = p(H_1)$ is the prior probability of a merge occuring for clusters $i$ and $j$. This makes the denominator of this expression the Bayesian *evidence*.

2) $H_2$ is the hypothesis that the two clusters $D_i$ and $D_j$ were generated from two independent distributions and therefore should *not* be joined together as cluster $D_k$. The probability of $H_2$ is calculated as $p(D_k|H_2) = p(D_i|T_i)p(D_j|T_j)$ where $T_i$ and $T_j$ are the subclusters being examined.

All existing clusters are compared and joined based on the combination of clusters with the highest posterior merge probability $r_k$. In our module this iterative action of comparing and merging clusters occurs in the `HierarchyTree` object. `HierarchyTree` is initialized by passing in the data matrix `X`, the parameters of the prior distributions, and the family of distributions. Currently, support for the Multivariate Normal-Inverse Wishart family exists with support for the Beta-Binomial and Multinomial-Dirichlet planned as the next steps in development of this module. The `HierarchyTree` is "grown" using the `grow_tree` method. This method is analogous to the `fit` method for a class in `sklearn`.

The BHC algorithm is initialized by setting all data points within their own singleton cluster represented by a "leaf" in what will become the clustered hierarchy tree. The algorithm iterates over all possible $n$ choose $2$ combinations of these one-point clusters and calculates the posterior possibility of each combination. The combination of clusters with the highest posterior probability are merged to form a new cluster in a "greedy" fashion. This concludes one iteration of the algorithm.

The algorithm repeats the steps described above for merging clusters until only one super-cluster of all the data points remains. Due to the greedy nature of the algorithm, this tree will have merged clusters with low posterior merge probabilities. These merges are considered *unjustified* and usually occur at the top of the tree. In the final step of the algorithm, unjustified merges seperated if their posterior merge probability is less than a user defined threshold.

To summarize the above in step-by-step form:

1) Vectors $\{x_1, ..., x_n\}$ are data points generated with distribution $p(x | \theta)$ with prior $p(\theta | \beta)$. These vectors are initialized into seperate clusters $\{1, ..., n\}$.

2) All possible combinations of the clusters are compared to find the highest posterior merge probability $r_k$. This combination is joined into a new cluster in a higher *tier* within the tree.

3) The number of clusters is then reduced by $1$, and the process repeats itself from step $2$ while the number of clusters is greater than $1$.

4) The tree is cut at points where the posterior probability of merging is less than some specified threshold. Typically the threshold value is chosen to be $r_k < 0.5$. 

## Optimization for Performance

After writing the code for the complete algorithm including the `HierarchyTree`, `Split`, and `Leaf` classes described in the *Background* section, we began looking for areas in which we could improve the speed of the BHC algorithm. It is important to note that, because of the algorithm requirement that *all* clusters be tested for merging with all other clusters, the computational burden of the algorithm grows extremely quickly with large $n$ datasets and datasets with high dimensions. As described above, Bayesian hierarchical clustering is quadratic in the number of datapoints in terms of complexity. 

One challenge of the BHC algorithm exists in the recursive calculation of $p(D_k|H_2)$. As the size of the dataset increases, the number of recursions must also increase as we continue to "grow" the `HierachyTree`. We quickly reach a point at which continued recursion becomes infeasible. To combat this problem we developed two additional classes for storing various information about a given cluster. An instance of the `Leaf` object is created for each data pont when the `HierarchyTree` is initialized. Once the `grow_tree` method is invoked instances of the `Split` object are created as clusters merge. The `Split` instances are designed to save the probability of a posterior merge. In this manner a true recursive calculation is unnecessary and the computing cost of the algorithm is greatly reduced.

We employed the `cProfile` package in order to find the rate limiting steps in the `grow_tree` method. Profiling our code showed us that, while our functions were themselves not very resource intensive, the algorithm requires that they be called many times during each iteration. Most notably the function for calculating the marginal likelihood of a cluster (i.e. the Bayesian evidence) `marginal_clust_k` was calling for several repeated calculations of the probability of merging clusters $p(D_k|H_k)$. Through some simple code restructuring we were able to reduce the number of times this calculation was made. This reduced our computational time from $12s$ to $8s$ on a toy data set of $30$ points. This translates to significant time savings on much larger data sets.

## Application to Simulated Data

In order to test our algorithm and its various components we generated three random multivariate Gaussian clusters of different sizes, means, and covariances:

Cluster $1$: $\mu_1:$, $\Sigma_1:$, $n:$ 

Cluster $2$: $\mu_2:$, $\Sigma_2:$, $n:$

Cluster $3$: $\mu_3:$, $\Sigma_3:$, $n:$



## Application to Real Data

## Comparative Analysis

### Comparison to Non-Bayesian Hierarchical Clustering

### Comparison to another algorithm (K-means?)

## Discussion and Conclusions



## References

*Heller, K. A., & Ghahramani, Z. (2005). Bayesian hierarchical clustering.
Proceedings of the 22nd International Conference on Machine Learning - ICML 05.*

*Rasmussen, C. E. (2000). The Infinite Gaussian Mixture Model. 
Advances in Neural Information Processing Systems 12, 554-560.*
