---
title: "STA 663 Final Project: Bayesian Hierarchical Clustering"
author: "Sam Voisin & Jonathan Klus"
date: "April 30, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Bayesian Hierarchical Clustering (BHC) is an agglomerative tree-based method for identifying underlying population structures ("clusters"). BHC was introduced by K. Heller and Z. Ghahramani as a way to approximate the more computationally intensive infinite Gaussian mixture model introduced by C. Rassmusen. The advantage of these models over their counterparts lies in the fact that an *ex ante* number of clusters does not need to be specified. Instead, the Bayesian approach allows for regularized flexibility via a prior placed on the cluster concentration parameter $\alpha$. We utilized the object-oriented programing (OOP) capabilities to build a module that can be used similar manner to the popular scikit-learn library. As with many Bayesian methods, the increased flexibility of BHC comes at a computational cost and the increased risk of poor results due to misspecified priors. Throughout this paper we will describe methods we used to mitigate these issues where possible.

## Background

The core of the BHC algorithm relies on a Bayesian hypothesis test in which two alternatives are compared:

1) $H_1$ is the hypothesis that two clusters $D_i$ and $D_j$ were generated from the same distribution $p(x | \theta)$ with the prior distribution for $\theta$ being $p(\theta | \beta)$. The probability of clusters $i$ and $j$ being generated from the same distribution is defined as $p(D_k|H_1)$. The posterior for this hypothesis is:

$$
r_k = \frac{\pi_k p(D_k|H_1)}{\pi_k p(D_k|H_1) + (1 - \pi_k) p(D_i|T_i)p(D_j|T_j)}
$$

Note that $\pi_k$ is the prior probability of a merge occuring for clusters $i$ and $j$. This makes the denominator of this expression the *evidence* in the Bayesian paradigm.

2) $H_2$ is the hypothesis that the two clusters $D_i$ and $D_j$ were generated from two independent distributions and therefore should *not* be joined together as cluster $D_k$. The probability of $H_2$ is calculated as $p(D_k|H_2) = p(D_i|T_i)p(D_j|T_j)$ where $T_i$ and $T_j$ are the subclusters being examined.

All existing clusters are compared and joined based on the cluster with the highest posterior merge probability $r_k$. In our module this iterative action of comparing and merging clusters occurs in the `HierarchyTree` object. `HierarchyTree` is initialized by passing in the data matrix `X`, the parameters of the prior distributions, and the family of distributions. Currently, support for the Multivariate Normal-Inverse Wishart family exists with support for the Beta-Binomial and Multinomial-Dirichlet planned as the next steps in development of this module. The `HierarchyTree` is "grown" using the `grow_tree` method. This method is analogous to the `fit` method for a class in `sklearn`.

One challenge of the BHC algorithm exists in the recursive calculation of $p(D_k|H_2)$. As the size of the dataset increases, the number of recursions must also increase as we continue to "grow" the `HierachyTree`. We quickly reach a point at which continued recursion becomes infeasible. To combat this problem we developed two additional classes for storing various information about a given cluster. An instance of the `Leaf` object is created for each data pont when the `HierarchyTree` is initialized. Once the `grow_tree` method is invoked instances of the `Split` object are created as clusters merge. The `Split` instances are designed to save the calculations used to evaluate each merge. In this manner a true recursive calculation is unnecessary and the time cost of the algorithm is greatly reduced.

## The Algorithm

The BHC algorithm is initialized by setting all data points as their own cluster represented by a "leaf" in what will become the clustered tree. The algorithm iterates over all possible $n$ choose $2$ combinations of these one point clusters and calculating the posterior possibility of the combinations being generated by the same distribution which is represented by $r_k$. The combination of clusters with the highest posterior probability are merged to form a new cluster in a "greedy" fashion. This concludes one iteration of the algorithm.

The algorithm repeats the above pattern until only one super-cluster of all the data points remains. Due to the greedy nature of the algorithm, this tree will have made clusters with very low posterior merge probabilities. These improper merges are split if they fall under some pre-determined threshold.

To summarize in step-by-step form:

1) Vectors $\{x_1, ..., x_n\}$ are data points generated with distribution $p(x | \theta)$ and prior $p(\theta | \beta)$. These vectors are initialized into seperate clusters $\{1, ..., n\}$

2) Combinations of the clusters are compared to find the highest posterior merge probability $r_k$. This combination is joined into a new cluster in a higher *tier* within the tree.

3) The number of clusters is then reduced by $1$, and the process repeats itself while the number of clusters is greater than $1$.

## Optimization for Performance

After writing the code for the complete algorithm inlcuding the `HierarchyTree`, `Split`, and `Leaf` classes described in the *Backgorund* section, we began looking for areas in which we could improve the speed of the BHC algorithm. It is important to note that, because of the algorithm requirement that *all* clusters be tested for merging with all other clusters, the computational burden of the algorithm grows extremely quickly with large $n$ datasets and datasets with high dimensions.

We employed the `cProfile` package in order to find the rate limiting steps in the `grow_tree` method. Profiling our code showed us that, while our functions were themselves not very resource intensive, the algorithm requires that they be called many times during each iteration. Most notably the function for calculating the marginal likelihood of a cluster (i.e. the Bayesian evidence) `marginal_clust_k` was calling for several repeated calculations of the probability of merging clusters $p(D_k|H_k)$. Through some simple code restructuring we were able to reduce the number of times this calculation was made. This reduced our computational time from $12s$ to $8s$ on a toy data set of $30$ points. This provided a significant time savings on much larger data sets.

## Application to Simulated Data

In order to test our algorithm and its various components we generated three random multivariate Gaussian clusters of different sizes, means, and covariances:

Cluster $1$: $\mu_1:$, $\Sigma_1:$, $n:$ 

Cluster $2$: $\mu_2:$, $\Sigma_2:$, $n:$

Cluster $3$: $\mu_3:$, $\Sigma_3:$, $n:$



## Application to Real Data

## Comparative Analysis

### Comparison to Non-Bayesian Hierarchical Clustering

### Comparison to another algorithm (K-means?)

## Discussion and Conclusions



## References

*Heller, K. A., & Ghahramani, Z. (2005). Bayesian hierarchical clustering.
Proceedings of the 22nd International Conference on Machine Learning - ICML 05.*

*Rasmussen, C. E. (2000). The Infinite Gaussian Mixture Model. 
Advances in Neural Information Processing Systems 12, 554-560.*
